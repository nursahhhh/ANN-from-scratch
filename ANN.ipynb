{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN deneme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\NURŞAH SATILMIŞ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Operation:\n",
    "  def __init__(self,d_loader):\n",
    "    self.d_loader=d_loader\n",
    "\n",
    "  def onehotencoding(self):\n",
    "    \"\"\"num_classes=len(np.unique(y_train))\n",
    "    y_train=np.array([np.insert(np.zeros(num_classes-1),i,1) for i in y_train]).astype(int) another way implemented by me\"\"\"\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    self.d_loader.y_train=enc.fit_transform(self.d_loader.y_train.reshape(self.d_loader.y_train.shape[0],1)).toarray().astype(int)\n",
    "    self.d_loader.y_test=enc.transform(self.d_loader.y_test.reshape(self.d_loader.y_test.shape[0],1)).toarray().astype(int)\n",
    "\n",
    "  def visualize(self):\n",
    "    for i in range(9):\n",
    "      plt.subplot(330 + 1 + i)\n",
    "      plt.imshow(self.d_loader.X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "  def transform_dimension(self):\n",
    "    self.d_loader.X_train = self.d_loader.X_train.astype('float32')\n",
    "    self.d_loader.X_test = self.d_loader.X_test.astype('float32')\n",
    "\n",
    "    self.d_loader.X_train=self.d_loader.X_train.reshape(self.d_loader.X_train.shape[0],-1)\n",
    "    self.d_loader.X_test=self.d_loader.X_test.reshape(self.d_loader.X_test.shape[0],-1)\n",
    "\n",
    "  def standardize(self):\n",
    "    self.d_loader.X_train=(self.d_loader.X_train-np.mean(self.d_loader.X_train,axis=0))/(np.std(self.d_loader.X_train,axis=0) + 10e-16) # we need epsilon for feature (pixel) that has zero variance\n",
    "    self.d_loader.X_test=(self.d_loader.X_test-np.mean(self.d_loader.X_test,axis=0))/(np.std(self.d_loader.X_test,axis=0) + 10e-16) # we need epsilon for feature (pixel) that has zero variance\n",
    "\n",
    "  def shuffle(self):\n",
    "    # Storing indeces of data\n",
    "    indices = np.arange(self.d_loader.X_train.shape[0])\n",
    "    # Shuffling the indeces\n",
    "    np.random.shuffle(indices)\n",
    "    # update the data and target with the new shuffled indeces\n",
    "    self.d_loader.X_train = self.d_loader.X_train[indices]\n",
    "    self.d_loader.y_train = self.d_loader.y_train[indices]\n",
    "    return self.d_loader\n",
    "tf.test.gpu_device_name()\n",
    "np.seterr(over='ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "class Data_Loader:\n",
    "  def __init__(self):\n",
    "    (self.X_train, self.y_train), (self.X_test, self.y_test) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "  def __init__(self,d_loader):\n",
    "    self.d_loader=d_loader\n",
    "\n",
    "  def onehotencoding(self):\n",
    "    \"\"\"num_classes=len(np.unique(y_train))\n",
    "    y_train=np.array([np.insert(np.zeros(num_classes-1),i,1) for i in y_train]).astype(int) another way implemented by me\"\"\"\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    self.d_loader.y_train=enc.fit_transform(self.d_loader.y_train.reshape(self.d_loader.y_train.shape[0],1)).toarray().astype(int)\n",
    "    self.d_loader.y_test=enc.transform(self.d_loader.y_test.reshape(self.d_loader.y_test.shape[0],1)).toarray().astype(int)\n",
    "\n",
    "  def visualize(self):\n",
    "    for i in range(9):\n",
    "      plt.subplot(330 + 1 + i)\n",
    "      plt.imshow(self.d_loader.X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "  def transform_dimension(self):\n",
    "    self.d_loader.X_train = self.d_loader.X_train.astype('float32')\n",
    "    self.d_loader.X_test = self.d_loader.X_test.astype('float32')\n",
    "\n",
    "    self.d_loader.X_train=self.d_loader.X_train.reshape(self.d_loader.X_train.shape[0],-1)\n",
    "    self.d_loader.X_test=self.d_loader.X_test.reshape(self.d_loader.X_test.shape[0],-1)\n",
    "\n",
    "  def standardize(self):\n",
    "    self.d_loader.X_train=(self.d_loader.X_train-np.mean(self.d_loader.X_train,axis=0))/(np.std(self.d_loader.X_train,axis=0) + 10e-16) # we need epsilon for feature (pixel) that has zero variance\n",
    "    self.d_loader.X_test=(self.d_loader.X_test-np.mean(self.d_loader.X_test,axis=0))/(np.std(self.d_loader.X_test,axis=0) + 10e-16) # we need epsilon for feature (pixel) that has zero variance\n",
    "\n",
    "  def shuffle(self):\n",
    "    # Storing indeces of data\n",
    "    indices = np.arange(self.d_loader.X_train.shape[0])\n",
    "    # Shuffling the indeces\n",
    "    np.random.shuffle(indices)\n",
    "    # update the data and target with the new shuffled indeces\n",
    "    self.d_loader.X_train = self.d_loader.X_train[indices]\n",
    "    self.d_loader.y_train = self.d_loader.y_train[indices]\n",
    "    return self.d_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation :\n",
    "\n",
    "    ''' The sigmoid function formula\n",
    "     σ(x) = 1 / (1 + e^(-x))'''\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    ''' \n",
    "     The softmax function formula for the i-th component of vector z\n",
    "    softmax(z)_i = exp(z_i) / sum(exp(z_j) for j in range(K))\n",
    "    '''\n",
    "    def softmax(self,Z) :\n",
    "        Z = np.exp(Z)/np.sum(np.exp(Z),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def relu(self , z):\n",
    "        if(z<=0):\n",
    "            z=0\n",
    "        else:\n",
    "            z=1\n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RMSProp (Root Mean Square Propagation)\n",
    "     The RMSProp optimizer works by maintaining a moving average of the squared gradient for each weight\n",
    "    -The RMSProp optimizer uses a decay rate/beta hyperparameter to control the weighting of past and current gradients.\n",
    "    -This decay rate determines how much weight is given to the most recent gradient compared to the historical gradients.\n",
    "    -A larger decay rate means that the moving average will be more heavily influenced by the historical gradients,\n",
    "    -while a smaller decay rate means that the moving average will be more influenced by the most recent gradient.\n",
    "### Vdw\n",
    "    -Vdw is the moving average of the squared gradients of the weights, and Vdb is the moving average of the squared gradients of the biases.\n",
    "    -These moving averages are used to adjust the learning rate during training, with the goal of improving the convergence of the optimization algorithm.\n",
    "\n",
    " -The letter \"V\" in Vdw and Vdb stands for \"variance\". In the context of the RMSProp optimizer, Vdw and Vdb represent the variance of the gradients of the weights and biases, respectively.\n",
    "    -The use of variance in the RMSProp optimizer is based on the intuition that the variance of the gradients can provide information about the scale of the gradients.\n",
    "    -If the variance of the gradients is high, it suggests that the gradients are large and the learning rate should be reduced to prevent overshooting the minimum. Conversely,\n",
    "    -if the variance of the gradients is low, it suggests that the gradients are small and the learning rate can be increased to speed up convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (ADAM):\n",
    "It is an adaptive learning rate optimization algorithm that combines the benefits of two other optimization algorithms, RMSProp and momentum.\n",
    "he Adam optimizer works by maintaining a moving average of the gradients and the squared gradients of the weights\n",
    "    -VdW = beta1 * VdW + (1 - beta1) * dL/dw\n",
    "    -SdW = beta2 * SdW + (1 - beta2) * (dL/dw)^2\n",
    "    -VdW_corrected = VdW_corrected / (1 - beta1^t)\n",
    "    -SdW_corrected = SdW_corrected / (1 - beta2^t)\n",
    "    -w = w - alpha * VdW_corrected / (sqrt(SdW_corrected) + epsilon)\n",
    "    -the same for b\n",
    "\n",
    "    beta1 and beta2 are hyperparameters\n",
    "    Set β1=0, means that ADAM behaves exactly as RMSprop optimizer\n",
    "    Set β2=0, means that ADAM behaves exactly as Momentum optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \n",
    "  def Initialize_RMS(self,beta,epsilon,Vdw,vdb):\n",
    "    return beta,epsilon,Vdw,vdb\n",
    "\n",
    "  def Initialize_Adam(self,beta1,beta2,epsilon,Vdw,Sdw,Vdb,Sdb):\n",
    "    return beta1,beta2,epsilon,Vdw,Sdw,Vdb,Sdb\n",
    "  def RMS(self,beta,epsilon,t,W,b,dw,db,VdW,Vdb,LR):\n",
    "    VdW = ((beta * VdW) + ((1 - beta) * (dw**2))) / (1 - beta**t + epsilon)\n",
    "    Vdb = ((beta * Vdb) + ((1 - beta) * (db**2))) / (1 - beta**t + epsilon)\n",
    "    W = W - (LR * (dw/(np.sqrt(VdW) + epsilon)))\n",
    "    b = b - (LR * (db/(np.sqrt(Vdb) + epsilon)))\n",
    "    return W,b\n",
    "  \n",
    "  \n",
    "  def Adam(self,beta1,beta2,t,epsilon,W,b,dw,db,VdW,SdW,Vdb,Sdb,eta):\n",
    "    VdW= (beta1 * VdW) + ((1 - beta1) * dw)    # Momentum\n",
    "    Vdb= (beta1 * Vdb) + ((1 - beta1) * db)    # Momentum\n",
    "    SdW= (beta2 * SdW) + ((1 - beta2) * dw**2) # RMS, represents variance that will be square rooted soon\n",
    "    Sdb= (beta2 * Sdb) + ((1 - beta2) * db**2) # RMS, represents variance that will be square rooted soon\n",
    "    \"\"\"\n",
    "    -The bias correction step helps to ensure that the moving averages are unbiased estimates of the true first and second moments of the gradients.\n",
    "    -Without the bias correction step, the moving averages would be biased towards zero, which could lead to slower convergence or suboptimal performance.\n",
    "    \"\"\"\n",
    "    VdW_corrected= VdW/(1 - beta1**t + epsilon) # bias correction\n",
    "    Vdb_corrected= Vdb/(1 - beta1**t + epsilon) # bias correction\n",
    "    SdW_corrected= SdW/(1 - beta2**t + epsilon) # bias correction\n",
    "    Sdb_corrected= Sdb/(1 - beta2**t + epsilon) # bias correction\n",
    "\n",
    "    W = W - eta * (VdW_corrected/np.sqrt(SdW_corrected + epsilon)) # SdW_corrected is standard deviation here\n",
    "    b = b - eta * (Vdb_corrected/np.sqrt(Sdb_corrected + epsilon)) # SdW_corrected is standard deviation here\n",
    "    return W,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron :\n",
    "    def __init__(self,num_of_layers,size_of_layers,epochs,LR,Tanh=False,Relu=False,batch_size=Data_Loader().X_train.shape[0],rms=False,adam=False):\n",
    "      d_loader=Data_Loader()\n",
    "      op=Operation(d_loader)\n",
    "      op.onehotencoding()\n",
    "      op.transform_dimension()\n",
    "      op.standardize()\n",
    "      d_loader=op.shuffle()\n",
    "      parameters=self.model_NN(d_loader.X_train,d_loader.y_train,num_of_layers,size_of_layers,epochs,LR,Tanh,Relu,batch_size,rms,adam)\n",
    "      print(self.test_model(parameters,d_loader.X_test,d_loader.y_test,Tanh,Relu))\n",
    "\n",
    "    def Accuracy(self,y_pred,y_test):\n",
    "      # argmax returns the index of the maximum element in a numpy array\n",
    "      Acc=np.mean(np.argmax(y_test, axis=1) == np.argmax(y_pred, axis=1),axis=0) * 100\n",
    "      return round(Acc,1)\n",
    "\n",
    "    def Error(self,y_pred,y_test):\n",
    "      return  (1/(2*y_test.shape[0])) * np.sum((y_pred-y_test)**2)\n",
    "\n",
    "    def layer_sizes(self,X,size_of_layers):\n",
    "      # Inserts the size of the input layer at the start of the array\n",
    "      size_of_layers.insert(0,X.shape[1])\n",
    "      return size_of_layers\n",
    "\n",
    "    def Print_shapes(self,container,param):\n",
    "      if param:\n",
    "        # Prints each W and each b between the layers\n",
    "        print('L',['layer '+str(i) for i in range(len(container[\"W\"])+1)])\n",
    "        print('W',[container['W'][i].shape for i in range(len(container[\"W\"]))])\n",
    "        print('b',[container['b'][i].shape for i in range(len(container[\"W\"]))])\n",
    "      else:\n",
    "        # Prints each dW and each db between the layers\n",
    "        print('L',['layer '+str(i) for i in range(len(container[\"dW\"])+1)])\n",
    "        print('dW',[container['dW'][i].shape for i in range(len(container[\"dW\"]))])\n",
    "        print('db',[container['db'][i].shape for i in range(len(container[\"dW\"]))])\n",
    "\n",
    "    def initialize_parameters(self,L_sizes):\n",
    "      W=[]\n",
    "      b=[]\n",
    "      # Inialize W to be size of next layer by size of current layer, Inialize b to be size of next layer by 1\n",
    "      for i in range(len(L_sizes)-1):\n",
    "        W.append(np.random.randn(L_sizes[i+1],L_sizes[i]))\n",
    "        b.append(np.zeros((L_sizes[i+1],1)))\n",
    "\n",
    "      parameters = {\n",
    "          'W':W,\n",
    "          'b':b\n",
    "      }\n",
    "\n",
    "      self.Print_shapes(parameters,True)\n",
    "\n",
    "      return parameters\n",
    "\n",
    "    def forward_propagation(self,X,parameters,Tanh,Relu):\n",
    "      act=activation()\n",
    "      Z=[]\n",
    "      A=[]\n",
    "\n",
    "      for i in range(len(parameters[\"W\"])):\n",
    "                                                                    # if we are at the first layer then we use X, O.W. we use A[L-1]\n",
    "        Z.append(np.dot(parameters[\"W\"][i],X.T) + parameters[\"b\"][i]) if i==0 else Z.append(np.dot(parameters[\"W\"][i],A[i-1]) + parameters[\"b\"][i])\n",
    "        if Tanh:\n",
    "          A.append(act.sigmoid(Z[i])) if i==len(parameters[\"W\"])-1 else A.append(np.tanh(Z[i]))\n",
    "        if Relu:\n",
    "          A.append(act.sigmoid(Z[i])) if i==len(parameters[\"W\"])-1 else A.append(np.maximum(0,Z[i]))\n",
    "        if (not Tanh) and (not Relu):                                                                                       # we check here if we can use dot product properly or do we need to transpose one of the arrays so the dot product could work\n",
    "          A.append(act.sigmoid(Z[i]))\n",
    "\n",
    "\n",
    "      cache = {\n",
    "          'Z':Z,\n",
    "          'A':A\n",
    "      }\n",
    "\n",
    "\n",
    "      return cache\n",
    "\n",
    "    def backward_propagation(self,parameters, cache, X, y,print_once,Tanh,Relu):\n",
    "      act=activation()\n",
    "      dZ=[]\n",
    "      dW=[]\n",
    "      db=[]\n",
    "      j=0\n",
    "\n",
    "      # we will move backward so I reversed the direction of the iterator by giving (no. layer-1, -1 which will make the iterator stop at 0,-1 which is responsible for reversing the direction)\n",
    "      for i in range(len(parameters[\"W\"])-1,-1,-1):\n",
    "      # if last layer then compute dL/dZ = A[L] - y\n",
    "        if i==len(parameters[\"W\"])-1:\n",
    "          dZ.append(cache['A'][i].T-y)\n",
    "      # else compute dL/dZ[L] = W[L+1] * dL/dZ[L+1] * dA/dZ[L], where dA/dZ[L] = A[L] * (1 - A[L])\n",
    "        else:\n",
    "          if Tanh:                                                                      # we check here if we can use dot product properly or do we need to transpose one of the arrays so the dot product could work\n",
    "            dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1].T) * (1 - cache[\"A\"][i]**2)) if parameters[\"W\"][i+1].T.shape[1]==dZ[j-1].T.shape[0] else dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1]) * (1 - cache[\"A\"][i]**2))\n",
    "          if Relu:\n",
    "            dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1].T) * act.der_Relu(cache[\"A\"][i])) if parameters[\"W\"][i+1].T.shape[1]==dZ[j-1].T.shape[0] else dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1]) * act.der_Relu(cache[\"A\"][i]))\n",
    "          if (not Tanh) and (not Relu):                                                                                       # we check here if we can use dot product properly or do we need to transpose one of the arrays so the dot product could work\n",
    "            dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1].T) * (cache[\"A\"][i] * (1 - cache[\"A\"][i]))) if parameters[\"W\"][i+1].T.shape[1]==dZ[j-1].T.shape[0] else dZ.append(np.dot(parameters[\"W\"][i+1].T,dZ[j-1]) * (cache[\"A\"][i] * (1 - cache[\"A\"][i])))\n",
    "      # if first layer then compute dL/dW[L] = dL/dZ[L] * X\n",
    "        if i==0:\n",
    "          dW.append(1/y.shape[0] * np.dot(dZ[j],X))\n",
    "      # else compute dL/dW[L] = dL/dZ[L] * A[L-1]\n",
    "        else:                                                         # we check here if we can use dot product properly or do we need to transpose one of the arrays so the dot product could work\n",
    "          dW.append(1/y.shape[0] * np.dot(dZ[j].T,cache['A'][i-1].T))  if dZ[j].T.shape[1]==cache['A'][i-1].T.shape[0] else  dW.append(1/y.shape[0] * np.dot(dZ[j],cache['A'][i-1].T))\n",
    "                                                                      # we make sure that the size of db be the same as b not (60000,1)\n",
    "        db.append(1/y.shape[0] * np.sum(dZ[j],axis=1,keepdims=True)) if np.sum(dZ[j],axis=1,keepdims=True).shape[0]!=X.shape[0] else db.append(1/y.shape[0] * np.sum(dZ[j].T,axis=1,keepdims=True))\n",
    "\n",
    "        j+=1\n",
    "\n",
    "      grads = {\n",
    "          'dW':dW,\n",
    "          'db':db\n",
    "      }\n",
    "\n",
    "      self.Print_shapes(grads,False) if print_once else \"\"\n",
    "\n",
    "      return grads\n",
    "\n",
    "\n",
    "    def update_parameters(self,parameters, grads, LR = 1.2,beta=0,beta1=0,beta2=0,j=0,epsilon=0,VdW=0,SdW=0,Vdb=0,Sdb=0,rms=False,adam=False):\n",
    "\n",
    "      optimizer=Optimizer()\n",
    "\n",
    "      for i in range(len(parameters[\"W\"])):\n",
    "        if rms:\n",
    "            parameters[\"W\"][i],parameters[\"b\"][i]=optimizer.RMS(beta,epsilon,j,parameters[\"W\"][i],parameters[\"b\"][i],grads['dW'][len(parameters[\"W\"])-1-i],grads['db'][len(parameters[\"W\"])-1-i],VdW[len(parameters[\"W\"])-1-i],Vdb[len(parameters[\"W\"])-1-i],LR)\n",
    "        if adam:\n",
    "            parameters[\"W\"][i],parameters[\"b\"][i]=optimizer.Adam(beta1,beta2,j,epsilon,parameters[\"W\"][i],parameters[\"b\"][i],grads['dW'][len(parameters[\"W\"])-1-i],grads['db'][len(parameters[\"W\"])-1-i],VdW[len(parameters[\"W\"])-1-i],SdW[len(parameters[\"W\"])-1-i],Vdb[len(parameters[\"W\"])-1-i],Sdb[len(parameters[\"W\"])-1-i],LR)\n",
    "        else:\n",
    "\n",
    "          # last value stored in dW and db are the gradients for the first W and b, that's why we are using parameters[\"W\"])-1-i\n",
    "            parameters[\"W\"][i]=parameters[\"W\"][i]-LR * grads['dW'][len(parameters[\"W\"])-1-i]\n",
    "            parameters[\"b\"][i]=parameters[\"b\"][i]-LR * grads['db'][len(parameters[\"W\"])-1-i]\n",
    "\n",
    "      return parameters\n",
    "\n",
    "    def train_model(self,X,y,batch_size,parameters,print_once,Tanh,Relu):\n",
    "      # applying forward propagation\n",
    "        cache=self.forward_propagation(X, parameters,Tanh,Relu)\n",
    "      # computing Error/Cost\n",
    "        cost=self.Error(cache['A'][-1].T,y)\n",
    "      # Computing gradients\n",
    "        grads=self.backward_propagation(parameters,cache,X,y,print_once,Tanh,Relu)\n",
    "        return cost,grads\n",
    "\n",
    "      # function to test on each model\n",
    "    def test_model(self,parameters,X_test,y_test,Tanh=False,Relu=False):\n",
    "      act=activation()\n",
    "      y_pred=np.zeros((X_test.shape[0],y_test.shape[1]))\n",
    "      # calling forward propagation to make predictions\n",
    "      cache=self.forward_propagation(X_test,parameters,Tanh,Relu)\n",
    "      # using softmax on y_pred\n",
    "      y_pred=act.softmax(cache['A'][-1].T)\n",
    "      return self.Accuracy(y_pred,y_test)\n",
    "\n",
    "    def model_NN(self,X, y,num_of_layers,size_of_layers,epochs,LR,Tanh,Relu,batch_size,rms,adam):\n",
    "      L_sizes=self.layer_sizes(X,size_of_layers)\n",
    "      parameters=self.initialize_parameters(L_sizes)\n",
    "      optimizer=Optimizer()\n",
    "\n",
    "      print_once=True\n",
    "      for j in range(epochs):\n",
    "\n",
    "        num_batches=int(X.shape[0]/batch_size)\n",
    "        for i in range(num_batches):\n",
    "            if rms:\n",
    "                cost,grads=self.train_model(X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],batch_size,parameters,print_once,Tanh,Relu)\n",
    "                print_once=False\n",
    "                beta,epsilon,VdW,Vdb = optimizer.Initialize_RMS(0.9,10e-16,[0] * num_of_layers,[0] * num_of_layers)\n",
    "                parameters=self.update_parameters(parameters,grads,LR=LR,beta=beta,j=j,epsilon=epsilon,VdW=VdW,Vdb=Vdb,rms=True)\n",
    "\n",
    "            if adam:\n",
    "                cost,grads=self.train_model(X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],batch_size,parameters,print_once,Tanh,Relu)\n",
    "                print_once=False\n",
    "                beta1,beta2,epsilon,VdW,SdW,Vdb,Sdb = optimizer.Initialize_Adam(0.9,0.999,10e-16,[0] * num_of_layers,[0] * num_of_layers,[0] * num_of_layers,[0] * num_of_layers)\n",
    "                parameters=self.update_parameters(parameters,grads,LR=LR,beta1=beta1,beta2=beta2,j=j,epsilon=epsilon,VdW=VdW,SdW=SdW,Vdb=Vdb,Sdb=Sdb,adam=True)\n",
    "\n",
    "            else:\n",
    "\n",
    "                if i != num_batches-1:\n",
    "                  cost,grads=self.train_model(X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],batch_size,parameters,print_once,Tanh,Relu)\n",
    "                  print_once=False\n",
    "                # Updating the parameters\n",
    "                  parameters=self.update_parameters(parameters,grads,LR=LR)\n",
    "                else:\n",
    "                  cost,grads=self.train_model(X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],batch_size,parameters,print_once,Tanh,Relu)\n",
    "                  print_once=False\n",
    "                # Updating the parameters\n",
    "                  parameters=self.update_parameters(parameters,grads,LR=LR)\n",
    "\n",
    "        if j % 50 == 0:\n",
    "              print (\"Cost after iteration %i: %f\" %(j, cost))\n",
    "\n",
    "      return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L ['layer 0', 'layer 1', 'layer 2']\n",
      "W [(20, 784), (10, 20)]\n",
      "b [(20, 1), (10, 1)]\n",
      "L ['layer 0', 'layer 1', 'layer 2']\n",
      "dW [(10, 20), (20, 784)]\n",
      "db [(10, 1), (20, 1)]\n",
      "Cost after iteration 0: 0.113361\n",
      "Cost after iteration 50: 0.042645\n",
      "Cost after iteration 100: 0.043651\n",
      "Cost after iteration 150: 0.040118\n",
      "Cost after iteration 200: 0.038675\n",
      "Cost after iteration 250: 0.037330\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2 layers => 1 hidden layer and 1 output layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mperceptron\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mperceptron.__init__\u001b[1;34m(self, num_of_layers, size_of_layers, epochs, LR, Tanh, Relu, batch_size, rms, adam)\u001b[0m\n\u001b[0;32m      8\u001b[0m d_loader\u001b[38;5;241m=\u001b[39mop\u001b[38;5;241m.\u001b[39mshuffle()\n\u001b[0;32m      9\u001b[0m parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_NN(d_loader\u001b[38;5;241m.\u001b[39mX_train,d_loader\u001b[38;5;241m.\u001b[39my_train,num_of_layers,size_of_layers,epochs,LR,Tanh,Relu,batch_size,rms,adam)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43md_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTanh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRelu\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[15], line 153\u001b[0m, in \u001b[0;36mperceptron.test_model\u001b[1;34m(self, parameters, X_test, y_test, Tanh, Relu)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# using softmax on y_pred\u001b[39;00m\n\u001b[0;32m    152\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mact\u001b[38;5;241m.\u001b[39msoftmax(cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m, in \u001b[0;36mperceptron.Accuracy\u001b[1;34m(self, y_pred, y_test)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAccuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m,y_pred,y_test):\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;66;03m# argmax returns the index of the maximum element in a numpy array\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m   Acc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(Acc,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NURŞAH SATILMIŞ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NURŞAH SATILMIŞ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\NURŞAH SATILMIŞ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2 layers => 1 hidden layer and 1 output layer\n",
    "perceptron(2,[20,10],300,1,batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
